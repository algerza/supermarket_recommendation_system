{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb38a87d",
   "metadata": {},
   "source": [
    "# 1. Import libraries and load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Build product metadata lookup, including sub-category and popularity\n",
    "df_meta = df[['product_id', 'title_en', 'category_translated', 'sub_category_translated', 'brand_name', 'popularity_rank', 'is_discount', 'main_delivery_city']]\n",
    "product_meta = df_meta.drop_duplicates('product_id').set_index('product_id')\n",
    "product_ids = sorted(product_meta.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733ad74",
   "metadata": {},
   "source": [
    "# 2. Create a data overview report\n",
    "\n",
    "\n",
    "**What is it?**  \n",
    "We use the `ydata_profiling` library to quickly generate a detailed report about the dataset. This includes summaries of each column (like missing values, unique counts, distributions) and insights into correlations and data quality issues.\n",
    "\n",
    "**Why it’s useful:**  \n",
    "This gives both technical and business users a full understanding of the data with minimal effort. You can find patterns, spot data quality problems, and understand what kind of information we have before building models.\n",
    "\n",
    "**How to use the output:**  \n",
    "Once the report is generated, scroll through the sections like \"Overview\", \"Variables\", and \"Interactions\". This helps you decide what cleaning or feature engineering steps might be needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b58f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create report using ydata_profiling\n",
    "profile = ProfileReport(df, title=\"Data Profiling Report\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebaf85b",
   "metadata": {},
   "source": [
    "# 3. Customer clusters\n",
    "\n",
    "\n",
    "**What happens in this section?**  \n",
    "We will group customers into **segments (clusters)** based on their behavior and product preferences. Clustering lets us identify different types of users, such as bargain hunters, brand loyalists, or health-conscious shoppers.\n",
    "\n",
    "**Why it matters for recommendations:**  \n",
    "These clusters help us build more **personalized and effective recommendations**. Instead of treating every user the same, we tailor suggestions based on their segment, improving user satisfaction and sales conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965bac92",
   "metadata": {},
   "source": [
    "## Generate features for K-Means clustering\n",
    "\n",
    "**Why go beyond just product_id and user_id?**  \n",
    "To cluster users meaningfully, we need to describe their behavior and preferences with **numerical features** (e.g., number of purchases in a category, average spend, discount sensitivity).\n",
    "\n",
    "**What’s the goal?**  \n",
    "We're converting complex purchase patterns into numbers that a machine learning algorithm (K-Means) can use to group similar users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c595f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average purchase price per customer\n",
    "avg_purchase_price = df.groupby(['customer_id', 'purchase_id'])['price'].sum().groupby('customer_id').mean()\n",
    "\n",
    "# Calculate average number of products per basket per customer\n",
    "avg_basket_product_number = df.groupby(['customer_id', 'purchase_id'])['product_id'].count().groupby('customer_id').mean()\n",
    "\n",
    "# Build interaction matrix (users × products)\n",
    "interaction = pd.crosstab(df['customer_id'], df['product_id']).reindex(columns=product_ids, fill_value=0)\n",
    "table = (interaction > 0).astype(int)\n",
    "\n",
    "# Create matrix with customer_id and product_id\n",
    "customer_ids = sorted(set(table.index) & set(avg_purchase_price.index) & set(avg_basket_product_number.index))\n",
    "table_with_price = table.loc[customer_ids]\n",
    "\n",
    "# Scale features\n",
    "avg_purchase_scaled = StandardScaler().fit_transform(avg_purchase_price.loc[customer_ids].values.reshape(-1, 1))\n",
    "avg_basket_scaled = StandardScaler().fit_transform(avg_basket_product_number.loc[customer_ids].values.reshape(-1, 1))\n",
    "\n",
    "# Create matrix for K-Means with customers, products and custom features\n",
    "enhanced_data_matrix = np.hstack([table_with_price.values, avg_purchase_scaled, avg_basket_scaled])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f64714",
   "metadata": {},
   "source": [
    "## Identify the optimal amount of clusters\n",
    "\n",
    "**Why this step matters:**  \n",
    "We don’t want to guess how many user segments to create. Instead, we use metrics like **Silhouette Score** to find the number of clusters that gives us the most meaningful and separate groupings.\n",
    "\n",
    "**How it works:**  \n",
    "The score evaluates how similar users are within a cluster vs. other clusters. A higher score means better-defined segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5efbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. User Segmentation & Optimal k Selection\n",
    "ks = range(2, 11) # Number of clusters to try\n",
    "inertias, silhouettes = [], []\n",
    "\n",
    "# Loop over 10 times the algorithm with different initial centroids and keep the best one\n",
    "#   Inertia measures compactness of clusters. The lower the better\n",
    "#   Silhouette measures quality/separation of clusters. The higher the better\n",
    "for k in ks:\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "    labels = km.fit_predict(enhanced_data_matrix)\n",
    "    inertias.append(km.inertia_)\n",
    "    # silhouette_score shows how similar a user is to its own cluster vs. other clusters. From -1 to 1\n",
    "    silhouettes.append(silhouette_score(enhanced_data_matrix, labels))\n",
    "\n",
    "# Plot metrics\n",
    "def plot_elbow_silhouette(ks, inertias, silhouettes):\n",
    "    fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(ks, inertias, 'o-', label='Inertia')\n",
    "    ax2.plot(ks, silhouettes, 's--', color='orange', label='Silhouette')\n",
    "    ax1.set_xlabel('k')\n",
    "    ax1.set_ylabel('Inertia')\n",
    "    ax2.set_ylabel('Silhouette Score')\n",
    "    plt.title('Elbow & Silhouette Analysis')\n",
    "    fig.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot elbow and silhouette\n",
    "plot_elbow_silhouette(ks, inertias, silhouettes)\n",
    "\n",
    "# Automatically select the silhouette number for clustering\n",
    "k_sil = ks[np.argmax(silhouettes)]\n",
    "\n",
    "# Elbow heuristic via second derivative\n",
    "d1 = np.diff(inertias)\n",
    "d2 = np.diff(d1)\n",
    "k_elbow = ks[np.argmax(d2) + 2]\n",
    "\n",
    "# Decide k (default to silhouette)\n",
    "n_clusters = k_sil\n",
    "\n",
    "# Fit K-Means with enhanced data\n",
    "kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "user_segments = pd.Series(kmeans.fit_predict(enhanced_data_matrix), index=customer_ids, name='segment')\n",
    "\n",
    "print(\"\\n=== Optimal Cluster Selection ===\")\n",
    "print(f\"• Optimal k by Silhouette Score : {k_sil} (score = {max(silhouettes):.3f})\")\n",
    "print(f\"• Optimal k by Elbow Heuristic  : {k_elbow}\")\n",
    "print(f\"• Final k selected for clustering: {n_clusters}\")\n",
    "\n",
    "print(\"\\n=== Cluster Membership ===\")\n",
    "print(\"Number of customers in each cluster:\")\n",
    "print(user_segments.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fea5b",
   "metadata": {},
   "source": [
    "## Plot clusters \n",
    "\n",
    "\n",
    "**What does this chart show?**  \n",
    "Imagine we had 50 different things describing a user — t-SNE compresses all that into a simple 2D map. Together, they help us ‘see’ the invisible patterns in user behavior. \n",
    "- **Component 1 (X-axis):** Represents one direction of user similarity  \n",
    "- **Component 2 (Y-axis):** Represents another direction  \n",
    "- Users that are close together behave similarly. Users far apart behave differently.\n",
    "\n",
    "**Why this helps:**  \n",
    "It lets us “see” hidden patterns in user behavior and check if our clusters make intuitive sense.\n",
    "\n",
    "**How to interpret the results:**  \n",
    "- Cluster separation: Are clusters clearly apart froim each other? If yes, that means that the clusters are easily differentiated\n",
    "- Cluster compactness: Are clusters \"tight\" or spread out? Meaning, if most of the data points or \"balls\" within the clusters are close to each other, it is a good sign, each customer is closely related to the rest od the cluster.\n",
    "- Cluster size: Are some clusters much bigger/smaller? If they are pretty balanced, then it's good. Otherwise we'd need to understand why (i.e. features) of the clusters\n",
    "- Outliers: Are there any random points far from all clusters? If no visible outliers, that means that customers of each cluster are similar between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66770ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions with t-SNE. This is good for visualization of our previous clustering.\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "reduced_tsne = tsne.fit_transform(enhanced_data_matrix)\n",
    "\n",
    "# Plot with custom legend\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Scatter each cluster individually to add labels\n",
    "for cluster_id in sorted(user_segments.unique()):\n",
    "    indices = (user_segments == cluster_id)\n",
    "    plt.scatter(\n",
    "        reduced_tsne[indices, 0],\n",
    "        reduced_tsne[indices, 1],\n",
    "        label=f\"Cluster {cluster_id}\",\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "plt.title(\"How similar are users between them? And clusters?\", fontsize=14)\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(title=\"User Segments\", loc='best', fontsize=9)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16ce55",
   "metadata": {},
   "source": [
    "## Create a report to understand our clusters\n",
    "\n",
    "\n",
    "**What are we looking for here?**  \n",
    "We analyze the characteristics of each customer cluster: what products they buy, which categories they prefer, how price-sensitive they are, etc.\n",
    "\n",
    "**Why this is important:**  \n",
    "This helps the business tailor marketing and product strategies to each group. For example, we might promote discounts more to the budget-conscious cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Add user segment information to each transaction\n",
    "df['segment'] = df['customer_id'].map(user_segments)\n",
    "df_seg = df[df['segment'].notna()]  # Keep only valid segments\n",
    "\n",
    "# 2. Basket Analysis: Avg products and spend per basket\n",
    "products_per_basket = df_seg.groupby(['segment', 'purchase_id'])['product_id'].count()\n",
    "avg_products_per_basket = products_per_basket.groupby('segment').mean().round(2)\n",
    "\n",
    "purchase_totals = df_seg.groupby(['segment', 'purchase_id'])['price'].sum()\n",
    "avg_purchase_price_by_segment = purchase_totals.groupby('segment').mean().round(2)\n",
    "\n",
    "basket_stats_by_segment = pd.DataFrame({\n",
    "    'Avg Products per Basket': avg_products_per_basket,\n",
    "    'Avg Purchase Price': avg_purchase_price_by_segment\n",
    "}).reset_index()\n",
    "\n",
    "print(\"\\n📦 Basket and Purchase Price by Segment:\\n\")\n",
    "print(tabulate(basket_stats_by_segment, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "\n",
    "# 3. Top 5 Products per Segment\n",
    "top_products_by_segment = (\n",
    "    df_seg.groupby(['segment', 'title_en'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .sort_values(['segment', 'count'], ascending=[True, False])\n",
    ")\n",
    "top_products = top_products_by_segment.groupby('segment').head(5)\n",
    "\n",
    "print(\"\\n🏆 Top 5 Products by Segment:\\n\")\n",
    "for segment, group in top_products.groupby('segment'):\n",
    "    print(f\"Cluster {segment}:\")\n",
    "    print(tabulate(group[['title_en', 'count']], headers=[\"Product\", \"Count\"], tablefmt='github', showindex=False))\n",
    "    print()\n",
    "\n",
    "# 4. Discount Usage by Segment\n",
    "discount_usage = df_seg.groupby('segment')['is_discount'].mean().round(2).reset_index()\n",
    "discount_usage.columns = ['Segment', 'Avg Discount Usage']\n",
    "\n",
    "print(\"\\n💸 Average Discount Usage by Segment:\\n\")\n",
    "print(tabulate(discount_usage, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "\n",
    "# 5. Subcategory Diversity per Segment\n",
    "subcategory_diversity = df_seg.groupby('segment')['sub_category_translated'].nunique().reset_index()\n",
    "subcategory_diversity.columns = ['Segment', 'Unique Subcategories']\n",
    "\n",
    "print(\"\\n🌈 Subcategory Diversity per Segment:\\n\")\n",
    "print(tabulate(subcategory_diversity, headers='keys', tablefmt='fancy_grid', showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14066fa8",
   "metadata": {},
   "source": [
    "# 4. Hybrid recommendation model \n",
    "\n",
    "**What is a hybrid model?**  \n",
    "It combines two approaches:\n",
    "- **Content-based filtering**: Recommends products similar to what the user liked before\n",
    "- **Collaborative filtering**: Recommends what similar users liked\n",
    "\n",
    "**Why this is powerful:**  \n",
    "It balances the strengths of both systems. Content-based ensures relevance; collaborative finds new things users may not discover on their own. This improves both **accuracy** and **discovery**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f371d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "###\n",
    "###      Build product descriptions for content-based similarity\n",
    "###\n",
    "###########################################################################\n",
    "\n",
    "# We're combining product name, sub-category and discount flag\n",
    "# into one text field that describes the \"essence\" of the product.\n",
    "product_features = product_meta[['title_en', 'category_translated', 'sub_category_translated', 'brand_name', 'is_discount']].copy()\n",
    "\n",
    "# Clean and prepare features: fill blanks and convert to string\n",
    "for col in ['title_en', 'brand_name', 'category_translated', 'sub_category_translated']:\n",
    "    product_features[col] = product_features[col].fillna('').astype(str)\n",
    "product_features['is_discount'] = product_features['is_discount'].fillna(0).astype(str)\n",
    "\n",
    "# Merge into one text string per product for machine learning to process\n",
    "product_features['features'] = (\n",
    "    product_features['title_en'] + ' ' +\n",
    "    product_features['sub_category_translated'] + ' ' +\n",
    "    product_features['brand_name'] + ' ' +\n",
    "    product_features['is_discount']\n",
    ")\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "###\n",
    "###               Calculate Content-Based Similarity\n",
    "###\n",
    "###########################################################################\n",
    "\n",
    "# TF-IDF (Term Frequency - Inverse Document Frequency) converts product descriptions into a numerical format.\n",
    "# Words that are unique to a product are given more importance.\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(product_features['features'])\n",
    "\n",
    "# Cosine similarity: compares each product with every other product based on their text descriptions\n",
    "# Cosine similarity = 1 means very similar, 0 means unrelated\n",
    "content_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Store results as a readable table (product vs product)\n",
    "content_sim_df = pd.DataFrame(content_sim, index=product_meta.index, columns=product_meta.index)\n",
    "\n",
    "# STEP 3: Calculate Collaborative Filtering Similarity\n",
    "# Here we compare products based on user behavior (who bought what), using the item-user matrix.\n",
    "# Two products are similar if bought by the same types of customers.\n",
    "item_user_matrix = table_with_price.T\n",
    "collab_sim = cosine_similarity(item_user_matrix)\n",
    "\n",
    "# Store collaborative similarity results\n",
    "collab_sim_df = pd.DataFrame(collab_sim, index=item_user_matrix.index, columns=item_user_matrix.index)\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "###\n",
    "###           Combine both approaches: Content + Collaborative\n",
    "###\n",
    "###########################################################################\n",
    "\n",
    "# Hybrid similarity = 60% content-based + 40% collaborative filtering\n",
    "# This captures both product characteristics and customer behavior\n",
    "def hybrid_similarity(content_sim, collab_sim, alpha=0.6):\n",
    "    return alpha * content_sim + (1 - alpha) * collab_sim\n",
    "\n",
    "hybrid_sim = hybrid_similarity(content_sim, collab_sim, alpha=0.6) # alpha: % content-based\n",
    "hybrid_sim_df = pd.DataFrame(hybrid_sim, index=product_meta.index, columns=product_meta.index)\n",
    "\n",
    "# Optional helper to normalize any product similarity vector (for ranking)\n",
    "def normalize(x):\n",
    "    norm = np.linalg.norm(x)\n",
    "    return x / norm if norm != 0 else x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37381b7",
   "metadata": {},
   "source": [
    "# 5. Define recommendation engine for different scenarios\n",
    "\n",
    "**Why go beyond just hybrid recommendations?**  \n",
    "Users behave differently in different contexts — we adapt recommendations based on the situation: first-time visitor, loyal buyer, or someone with a full cart.\n",
    "\n",
    "**What this section does:**  \n",
    "We build **dynamic recommendation logic** for different scenarios. For example:\n",
    "- Use segment-based suggestions for new users\n",
    "- Personalize using collaborative scores for loyal users\n",
    "\n",
    "**Business benefit:**  \n",
    "Improves engagement by meeting users where they are in the buying journey, instead of using a one-size-fits-all model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea1058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend top N products for a given segment using hybrid similarity\n",
    "def recommend_for_segment(seg_idx, top_n=5, alpha=0.6, oversample=5):\n",
    "    users = user_segments[user_segments == seg_idx].index\n",
    "    seg_vec = table.loc[users].mean(axis=0).values  # average preferences of segment\n",
    "\n",
    "    # Hybrid score: weighted average of content and collaborative filtering\n",
    "    hybrid_scores = alpha * normalize(seg_vec @ content_sim) + (1 - alpha) * normalize(seg_vec @ collab_sim)\n",
    "    idxs = np.argsort(hybrid_scores)[::-1][:top_n * oversample]  # Oversample for diversity\n",
    "    candidate_ids = [product_ids[i] for i in idxs]\n",
    "\n",
    "    # Promote diversity by selecting products from different subcategories\n",
    "    diverse = []\n",
    "    seen_sub = set()\n",
    "    for pid in candidate_ids:\n",
    "        sub = product_meta.loc[pid, 'sub_category_translated']\n",
    "        if sub not in seen_sub:\n",
    "            diverse.append(pid)\n",
    "            seen_sub.add(sub)\n",
    "        if len(diverse) >= top_n * 2:\n",
    "            break\n",
    "\n",
    "    # Sort final recommendations by popularity\n",
    "    final_sorted = sorted(diverse[:top_n], key=lambda pid: product_meta.loc[pid, 'popularity_rank'])\n",
    "    return [(pid, product_meta.loc[pid, 'title_en']) for pid in final_sorted]\n",
    "\n",
    "\n",
    "# Recommend top N products for an individual user\n",
    "def recommend_for_user(user_id, top_n=5, alpha=0.6, oversample=5):\n",
    "    if user_id not in table.index:\n",
    "        # Cold Start: If user has no previous purchase history, fallback to a default recommendation\n",
    "        print(f\"New user detected: {user_id}, recommending based on user segment or general popularity.\")\n",
    "        \n",
    "        # Option 1: Fallback to segment-based recommendation for a new user (assuming segment is assigned)\n",
    "        if user_id in user_segments:\n",
    "            seg = user_segments[user_id]\n",
    "            return recommend_for_segment(seg, top_n=top_n, alpha=alpha)\n",
    "        \n",
    "        # Option 2: If no segment available, fallback to the most popular products\n",
    "        print(\"No segment found for new user, recommending based on overall popularity.\")\n",
    "        popular_products = product_meta.sort_values(by='popularity_rank', ascending=True).head(top_n)\n",
    "        return [(pid, popular_products.loc[pid, 'title_en']) for pid in popular_products.index]\n",
    "\n",
    "    # Regular recommendation for known users\n",
    "    user_purchased = table.loc[user_id]\n",
    "    purchased_ids = user_purchased[user_purchased > 0].index.tolist()\n",
    "\n",
    "    # Extract user's preferred subcategories from past purchases\n",
    "    user_subs = set(product_meta.loc[purchased_ids, 'sub_category_translated'].dropna())\n",
    "    if not user_subs:\n",
    "        user_subs = set(product_meta['sub_category_translated'])  # default to all if none\n",
    "\n",
    "    # Recommend more items for the user's segment (oversampled to allow filtering)\n",
    "    seg = user_segments[user_id]\n",
    "    cands = [pid for pid, _ in recommend_for_segment(seg, top_n=top_n * oversample, alpha=alpha)]\n",
    "    cands = [pid for pid in cands if pid not in purchased_ids]  # remove already purchased\n",
    "\n",
    "    # Ensure diversity by selecting products from different subcategories\n",
    "    diverse = []\n",
    "    seen_sub = set()\n",
    "    for pid in cands:\n",
    "        sub = product_meta.loc[pid, 'sub_category_translated']\n",
    "        if sub not in seen_sub:\n",
    "            diverse.append(pid)\n",
    "            seen_sub.add(sub)\n",
    "        if len(diverse) >= top_n * 2:\n",
    "            break\n",
    "\n",
    "    # Score recommendations using hybrid similarity with the user's profile\n",
    "    rec_idxs = [product_ids.index(pid) for pid in diverse]\n",
    "    user_vec = table.loc[user_id].values\n",
    "    scores = user_vec @ hybrid_sim[:, rec_idxs]\n",
    "    top_local = np.argsort(scores)[::-1][:top_n]\n",
    "    final = [diverse[i] for i in top_local]\n",
    "\n",
    "    # Return final sorted list by product popularity\n",
    "    final_sorted = sorted(final, key=lambda pid: product_meta.loc[pid, 'popularity_rank'])\n",
    "    return [(pid, product_meta.loc[pid, 'title_en']) for pid in final_sorted]\n",
    "\n",
    "\n",
    "# Recommend products based on the current shopping cart\n",
    "def recommend_for_cart_and_user(cart_products, user_id=None, top_n=5, alpha=0.6, beta=0.5):\n",
    "    cart_vec = np.zeros(len(product_ids))\n",
    "    c_idxs = [product_ids.index(p) for p in cart_products if p in product_ids]\n",
    "    cart_vec[c_idxs] = 1\n",
    "\n",
    "    # Compute similarity scores based on cart\n",
    "    cart_cf = cart_vec @ collab_sim\n",
    "    cart_cb = cart_vec @ content_sim\n",
    "    cart_hybrid = alpha * normalize(cart_cb) + (1 - alpha) * normalize(cart_cf)\n",
    "\n",
    "    if user_id is not None and user_id in table.index:\n",
    "        user_vec = table.loc[user_id].values\n",
    "        user_hybrid = user_vec @ hybrid_sim\n",
    "        hybrid = beta * normalize(cart_hybrid) + (1 - beta) * normalize(user_hybrid)\n",
    "    else:\n",
    "        # Cold Start: If user unknown, fallback to cart-only recommendations\n",
    "        print(f\"New user detected or no user info, recommending based on cart only.\")\n",
    "        hybrid = cart_hybrid\n",
    "\n",
    "    # Rank candidates and remove items already in cart\n",
    "    idxs = np.argsort(hybrid)[::-1]\n",
    "    candidates = [product_ids[i] for i in idxs if product_ids[i] not in cart_products]\n",
    "\n",
    "    # Promote diversity by recommending different subcategories\n",
    "    diverse = []\n",
    "    seen_sub = set()\n",
    "    for pid in candidates:\n",
    "        sub = product_meta.loc[pid, 'sub_category_translated']\n",
    "        if sub not in seen_sub:\n",
    "            diverse.append(pid)\n",
    "            seen_sub.add(sub)\n",
    "        if len(diverse) >= top_n * 2:\n",
    "            break\n",
    "\n",
    "    # Return top N recommendations sorted by popularity\n",
    "    final_sorted = sorted(diverse[:top_n], key=lambda pid: product_meta.loc[pid, 'popularity_rank'])\n",
    "    return [(pid, product_meta.loc[pid, 'title_en']) for pid in final_sorted]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe18cb9",
   "metadata": {},
   "source": [
    "# Test our different scenarios\n",
    "\n",
    "**Why this is crucial:**\n",
    "This is the final check. Testing different user types ensures our system responds appropriately and delivers quality recommendations across all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test segment recommendation\n",
    "segment_id = 4\n",
    "segment_recommendations = recommend_for_segment(seg_idx=segment_id, top_n=10, alpha=0.6)\n",
    "\n",
    "print(f\"Personalized Recommendations for Segment: {segment_id}\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for idx, (pid, title) in enumerate(segment_recommendations, 1):\n",
    "    print(f\"{idx}. Product ID: {pid} | Title: {title}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Test user recommendation\n",
    "test_user = 'F7811111115'\n",
    "user_recommendations = recommend_for_user(test_user, top_n=10, alpha=0.7)\n",
    "\n",
    "print(f\"Personalized Recommendations for User: {test_user}\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for idx, (pid, title) in enumerate(user_recommendations, 1):\n",
    "    print(f\"{idx}. Product ID: {pid} | Title: {title}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Test cart-based recommendation\n",
    "test_cart = [1177, 1031, 612]  # example product_ids\n",
    "cart_recommendations = recommend_for_cart_and_user(test_cart, user_id='F7811111115', top_n=10, alpha=0.6, beta=0.5)\n",
    "\n",
    "print(f\"Recommendations Based on Cart: {test_cart}, for user 'F7811111115'\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for idx, (pid, title) in enumerate(cart_recommendations, 1):\n",
    "    print(f\"{idx}. Product ID: {pid} | Title: {title}\")\n",
    "print(\"\\n\")\n",
    "    \n",
    "\n",
    "# Test user recommendation\n",
    "test_user = '60176ce3-aa88-437a-9673-dd0262cdf560'\n",
    "user_recommendations = recommend_for_user(test_user, top_n=10, alpha=0.7)\n",
    "\n",
    "print(f\"Personalized Recommendations for User: {test_user}\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for idx, (pid, title) in enumerate(user_recommendations, 1):\n",
    "    print(f\"{idx}. Product ID: {pid} | Title: {title}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Test cart-based recommendation\n",
    "test_cart = [1086, 468, 612]  # example product_ids\n",
    "cart_recommendations = recommend_for_cart_and_user(test_cart, user_id='60176ce3-aa88-437a-9673-dd0262cdf560', top_n=10, alpha=0.6, beta=0.5)\n",
    "\n",
    "print(f\"Recommendations Based on Cart: {test_cart}\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for idx, (pid, title) in enumerate(cart_recommendations, 1):\n",
    "    print(f\"{idx}. Product ID: {pid} | Title: {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f60b5",
   "metadata": {},
   "source": [
    "# Identify potential data drift\n",
    "\n",
    "**Why this is crucial:**\n",
    "The economic environment or user preferences can change, and therefore, we need to keep an eye on how well we recommend products to our customers. If the recommendations are not meaningful anymore, we are losing potential to increase our revenue and customer retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "###\n",
    "###               Create a random synthetic dataset\n",
    "###\n",
    "###########################################################################\n",
    "\n",
    "def generate_random_dataframe(original_df, n_rows=1000):\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    \n",
    "    def sample_column(col):\n",
    "        if pd.api.types.is_numeric_dtype(original_df[col]):\n",
    "            return np.random.choice(original_df[col].dropna().unique(), size=n_rows)\n",
    "        else:\n",
    "            return np.random.choice(original_df[col].dropna().unique(), size=n_rows)\n",
    "    \n",
    "    random_df = pd.DataFrame({\n",
    "        'customer_id': sample_column('customer_id'),\n",
    "        'purchase_id': np.random.randint(low=1, high=1_000_000, size=n_rows),  # simulate new purchases\n",
    "        'product_id': sample_column('product_id'),\n",
    "        'title_en': sample_column('title_en'),\n",
    "        'price': np.round(np.random.normal(loc=original_df['price'].mean(), scale=original_df['price'].std(), size=n_rows), 2),\n",
    "        'category_translated': sample_column('category_translated'),\n",
    "        'sub_category_translated': sample_column('sub_category_translated'),\n",
    "        'brand_name': sample_column('brand_name'),\n",
    "        'popularity_rank': np.random.randint(1, 10, size=n_rows),  # assuming rank is between 1 and 10\n",
    "        'is_discount': np.random.choice([0.0, 1.0], size=n_rows, p=[0.7, 0.3]),\n",
    "        'main_delivery_city': sample_column('main_delivery_city'),\n",
    "        'segment': sample_column('segment'),\n",
    "    })\n",
    "\n",
    "    return random_df\n",
    "\n",
    "# Generate synthetic dataset\n",
    "df_new = generate_random_dataframe(df, n_rows=500)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "###\n",
    "###               Run Kolmogorov-Smirnov test\n",
    "###\n",
    "###########################################################################\n",
    "\n",
    "# Safety check: ensure the new data isn't empty\n",
    "if df_new.empty:\n",
    "    print(\"No new data found for drift detection.\")\n",
    "else:\n",
    "    # Calculate average purchase price per customer\n",
    "    avg_purchase_price_new = df_new.groupby(['customer_id', 'purchase_id'])['price'].sum().groupby('customer_id').mean()\n",
    "\n",
    "    # Calculate average number of products per basket per customer\n",
    "    avg_basket_product_number_new = df_new.groupby(['customer_id', 'purchase_id'])['product_id'].count().groupby('customer_id').mean()\n",
    "\n",
    "    # Build interaction matrix (users × products)\n",
    "    interaction_new = pd.crosstab(df_new['customer_id'], df_new['product_id']).reindex(columns=product_ids, fill_value=0)\n",
    "    table_new = (interaction_new > 0).astype(int)\n",
    "\n",
    "    # Align customers and scale features\n",
    "    customer_ids_new = sorted(set(table_new.index) & set(avg_purchase_price_new.index) & set(avg_basket_product_number_new.index))\n",
    "    table_with_price_new = table_new.loc[customer_ids_new]\n",
    "\n",
    "    # Apply same scaling as original\n",
    "    avg_purchase_scaled_new = StandardScaler().fit_transform(avg_purchase_price_new.loc[customer_ids_new].values.reshape(-1, 1))\n",
    "    avg_basket_scaled_new = StandardScaler().fit_transform(avg_basket_product_number_new.loc[customer_ids_new].values.reshape(-1, 1))\n",
    "\n",
    "    # Create enhanced feature matrix\n",
    "    new_data_matrix = np.hstack([table_with_price_new.values, avg_purchase_scaled_new, avg_basket_scaled_new])\n",
    "\n",
    "    def compute_distances_to_centroids(data, model):\n",
    "        \"\"\"Compute distances to KMeans centroids\"\"\"\n",
    "        distances = np.linalg.norm(data[:, None] - model.cluster_centers_, axis=2)\n",
    "        return distances.min(axis=1)\n",
    "\n",
    "    baseline_distances = compute_distances_to_centroids(enhanced_data_matrix, kmeans)\n",
    "    new_distances = compute_distances_to_centroids(new_data_matrix, kmeans)\n",
    "\n",
    "    # Run Kolmogorov-Smirnov test\n",
    "    ks_stat, p_value = ks_2samp(baseline_distances, new_distances)\n",
    "\n",
    "    print(f\"KS Statistic: {ks_stat:.4f}\")\n",
    "    if ks_stat > 0.03:\n",
    "        print(\"⚠️ Drift Detected: Significant change in distribution (KS statistic >= 0.5)\")\n",
    "    else:\n",
    "        print(\"✅ No Drift Detected: Distributions are within expected range (KS statistic < 0.3)\")\n",
    "\n",
    "    print(f\"P-Value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"⚠️ Drift Detected: Significant change in distribution (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"✅ No Drift Detected: Distributions are similar (p >= 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb229cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "###\n",
    "###               Run Adjusted Rand Index (ARI)\n",
    "###\n",
    "###########################################################################\n",
    "\n",
    "# Define our new and old data\n",
    "X_ref = enhanced_data_matrix[:split_point]\n",
    "X_current = new_data_matrix[split_point:]\n",
    "\n",
    "# Fit KMeans on the reference period\n",
    "kmeans_ref = KMeans(n_clusters=5, random_state=42, n_init=10).fit(X_ref)\n",
    "ref_labels = kmeans_ref.labels_\n",
    "\n",
    "# Predict cluster labels on the current data using the same model\n",
    "current_labels = kmeans_ref.predict(X_current)\n",
    "\n",
    "# Re-cluster current data independently for comparison\n",
    "kmeans_current = KMeans(n_clusters=5, random_state=42, n_init=10).fit(X_current)\n",
    "alt_current_labels = kmeans_current.labels_\n",
    "\n",
    "# Calculate ARI between reference clusters and current (re-clustered) ones\n",
    "ari_score = adjusted_rand_score(current_labels, alt_current_labels)\n",
    "print(f\"Adjusted Rand Index (ARI) for Drift Monitoring: {ari_score:.4f}\")\n",
    "\n",
    "if ari_score < 0:\n",
    "    print(f\"❌ Severe Drift Detected: Clustering structures strongly disagree (ARI = {ari_score:.4f} < 0)\")\n",
    "elif ari_score < 0.3:\n",
    "    print(f\"⚠️ Drift Detected: Significant change in clustering structure (ARI = {ari_score:.4f} < 0.3)\")\n",
    "else:\n",
    "    print(f\"✅ No Drift Detected: Clustering structure is stable (ARI = {ari_score:.4f} ≥ 0.3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49385326",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "###\n",
    "###               Visualise new clusters vs old ones\n",
    "###\n",
    "###########################################################################\n",
    "\n",
    "new_labels = kmeans.predict(new_data_matrix)\n",
    "\n",
    "# Combine data\n",
    "combined_matrix = np.vstack([enhanced_data_matrix, new_data_matrix])\n",
    "combined_cluster_labels = np.concatenate([kmeans.labels_, new_labels])\n",
    "combined_data_labels = ['Baseline'] * len(enhanced_data_matrix) + ['New'] * len(new_data_matrix)\n",
    "\n",
    "# t-SNE for 2D visualisation\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_result = tsne.fit_transform(combined_matrix)\n",
    "\n",
    "# Plotting clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "unique_clusters = np.unique(combined_cluster_labels)\n",
    "\n",
    "# Color map for consistency\n",
    "colors = plt.cm.tab10.colors  # up to 10 clusters\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    # Get baseline users in this cluster\n",
    "    idx_base = [i for i, (lbl, src) in enumerate(zip(combined_cluster_labels, combined_data_labels))\n",
    "                if lbl == cluster and src == 'Baseline']\n",
    "    if idx_base:\n",
    "        plt.scatter(tsne_result[idx_base, 0], tsne_result[idx_base, 1],\n",
    "                    label=f\"Cluster {cluster} (Baseline)\",\n",
    "                    alpha=0.7, s=60, marker='o', color=colors[cluster % len(colors)])\n",
    "    \n",
    "    # Get new users in this cluster\n",
    "    idx_new = [i for i, (lbl, src) in enumerate(zip(combined_cluster_labels, combined_data_labels))\n",
    "               if lbl == cluster and src == 'New']\n",
    "    if idx_new:\n",
    "        plt.scatter(tsne_result[idx_new, 0], tsne_result[idx_new, 1],\n",
    "                    label=f\"Cluster {cluster} (New)\",\n",
    "                    alpha=0.5, s=60, marker='s', color=colors[cluster % len(colors)])\n",
    "\n",
    "plt.title(\"t-SNE of User Clusters: Baseline vs New\", fontsize=14)\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"New user cluster counts:\", Counter(new_labels))\n",
    "print(\"Old user cluster counts:\", Counter(user_segments))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
